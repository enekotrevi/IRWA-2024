{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a4761ca-7735-4608-800a-feb31318803b",
   "metadata": {},
   "source": [
    "## IRWA Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e116d0f-223d-4766-9154-4c8fe02bcad5",
   "metadata": {},
   "source": [
    "#### Load Python packages\n",
    "Let's first import all the packages that we will need during the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff34c503-9e09-44d2-99a9-38dccaa6e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bertamitjavilapita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bertamitjavilapita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3566a9cd-bfd3-44ef-afb7-ddabe093088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a5588b-fb24-4931-a0d5-7ee6d0b2f52c",
   "metadata": {},
   "source": [
    "### Part 1: Text Processing and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ac0a0a-1f41-46a3-90f0-3c883aa2dacd",
   "metadata": {},
   "source": [
    "#### Load data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df77bd-14f8-4fc2-845a-f75032b6b409",
   "metadata": {},
   "source": [
    "**Data:** You are provided with a document corpus which is a set of tweets related to the Farmers\r",
    "Protests 2021. We also have the tweet_document_ids_map.csv that map Tweet IDs to Document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9a28edc4-46fc-4ad6-a3d9-6c93238925f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8f8e9078-04d7-4a73-a415-496b1a3d0b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + 'farmers-protest-tweets.json') as fp:\n",
    "    tweets = fp.readlines()\n",
    "tweets = [t.strip().replace(' +', ' ') for t in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3dce3fbc-b9b5-4e61-afc8-1157d1e9fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_document_ids_map = pd.read_csv(data_path + 'tweet_document_ids_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "9f174e22-8ae4-4d43-b080-2810bb0acd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to a dictionary\n",
    "# tweet_document_ids_map = tweet_document_ids_map.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fd6621-75d0-4875-bde3-92485ec85044",
   "metadata": {},
   "source": [
    "**1. Pre-process the documents.**\n",
    "- Removing stop words\r",
    "- Tokenization\r",
    "- Removing punctuation marks - FALTA\r",
    "- Stemming\r",
    "- and... anything else you think it's needed (bonus point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "efb94630-d50d-47c0-b38e-adebc7d5ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_terms(tweet):\n",
    "    \"\"\"\n",
    "    Preprocess the tweet text removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "    Argument:\n",
    "    tweet -- string (text) to be preprocessed\n",
    "\n",
    "    Returns:\n",
    "    tweet - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    # define the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # define the stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # transform the line to lowercase\n",
    "    tweet =  tweet.lower() \n",
    "\n",
    "    # delete punctioation\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    \n",
    "    # tokenize the text to get a list of terms\n",
    "    tweet = tweet.split()\n",
    "    \n",
    "    #eliminate the stopwords\n",
    "    tweet = [word for word in tweet if word not in stop_words]\n",
    "\n",
    "    # perform stemming\n",
    "    tweet = [stemmer.stem(word) for word in tweet]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e0743d33-29be-4d2f-bb4d-cbe3bc1711e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[304], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# tweet  \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Tweet | Date | Hashtags| Likes | Retweets | Url\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:  \n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# extract relevant information - we will use for the query (hint 1)     \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     tweet_id \u001b[38;5;241m=\u001b[39m tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m     tweet_content \u001b[38;5;241m=\u001b[39m tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m     tweet_date \u001b[38;5;241m=\u001b[39m tweet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "processed_tweets = []\n",
    "# tweet  \n",
    "# Tweet | Date | Hashtags| Likes | Retweets | Url\n",
    "\n",
    "for tweet in tweets:  \n",
    "    \n",
    "    # extract relevant information - we will use for the query (hint 1)     \n",
    "    tweet_id = tweet['tweet']\n",
    "    tweet_content = tweet['content']\n",
    "    tweet_date = tweet['date']\n",
    "    tweet_hastags = re.findall(r'#\\w+', tweet_content)\n",
    "    tweet_likes = tweet['likeCount']\n",
    "    tweet_retweets = tweet['retweetCount']\n",
    "    tweet_url = tweet['url']\n",
    "\n",
    "    # process the content\n",
    "    tweet_content = build_terms(tweet_content)\n",
    "    print(process_tweet)\n",
    "\n",
    "    # relevant info \n",
    "    formatted_output = f\"{tweet_id} | {tweet_content} | {tweet_date} | {tweet_hastags} | {tweet_retweets} | {tweet_likes} | {tweet_url}\"\n",
    "    processed_tweets.append(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fd466-4860-4dab-a83c-413e4fc99233",
   "metadata": {},
   "source": [
    "**2. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130707c4-632f-4e8b-94b7-d7ae50a0fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tweets in the corpus are: {}\".format(len(tweets)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
